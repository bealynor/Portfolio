{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUSA8001 - Assignment 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Points**: 100  \n",
    "**Due Date**: Friday Week 7 (5 April 2024) @ 11.59pm  \n",
    "**Instructions**: Provide answers within this Jupyter notebook using Python code, and submit via iLearn  \n",
    "**Marking Criteria**: See end of document below   \n",
    "**Assignment Background**: This assignment uses a dataset which is based on the Credit Card Defaults data discussed in Week 2 Tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "**Problem 1** - Reading the dataset (Total Marks: 20)\n",
    "\n",
    "\n",
    "\n",
    "**Q1**. Read the first 10,000 rows from the credit card dataset provided in the **assignment_data** folder \n",
    "- Name your DataFrame `df` \n",
    "- Delete `ID` column     \n",
    "\n",
    "(5 marks) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>310000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84373.0</td>\n",
       "      <td>57779.0</td>\n",
       "      <td>14163.0</td>\n",
       "      <td>8295.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2828.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45975.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>43987.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46257.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>43987.0</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40748.0</td>\n",
       "      <td>39816.0</td>\n",
       "      <td>40607.0</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22448.0</td>\n",
       "      <td>15490.0</td>\n",
       "      <td>17343.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>90000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28847.0</td>\n",
       "      <td>28747.0</td>\n",
       "      <td>29177.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>250000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>60000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3859.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>144047.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>73000.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>50136.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>50000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>-205.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>70000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47562.0</td>\n",
       "      <td>43735.0</td>\n",
       "      <td>43509.0</td>\n",
       "      <td>3020.0</td>\n",
       "      <td>5040.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LIMIT_BAL  SEX  EDUCATION  MARRIAGE   AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0        310000  1.0        3.0       1.0  32.0    0.0    0.0    0.0    0.0   \n",
       "1         10000  2.0        3.0       1.0  49.0   -1.0   -1.0   -2.0   -1.0   \n",
       "2         50000  1.0        2.0       1.0  28.0   -1.0   -1.0   -1.0    0.0   \n",
       "3         80000  2.0        3.0       1.0  52.0    2.0    2.0    3.0    3.0   \n",
       "4        270000  1.0        1.0       2.0  34.0    1.0    2.0    0.0    0.0   \n",
       "...         ...  ...        ...       ...   ...    ...    ...    ...    ...   \n",
       "9995      90000  2.0        2.0       1.0  47.0    0.0    0.0    0.0    0.0   \n",
       "9996     250000  2.0        1.0       2.0  26.0   -2.0   -2.0   -2.0   -2.0   \n",
       "9997      60000  2.0        2.0       1.0  31.0   -1.0   -1.0   -1.0   -1.0   \n",
       "9998      50000  1.0        2.0       2.0  24.0    1.0   -1.0   -1.0   -1.0   \n",
       "9999      70000  2.0        1.0       2.0  29.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "      PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0       0.0  ...    84373.0    57779.0    14163.0    8295.0    6000.0   \n",
       "1       2.0  ...     1690.0     1138.0      930.0       0.0       0.0   \n",
       "2      -1.0  ...    45975.0     1300.0    43987.0       0.0   46257.0   \n",
       "3       3.0  ...    40748.0    39816.0    40607.0    3700.0    1600.0   \n",
       "4       2.0  ...    22448.0    15490.0    17343.0       0.0    4000.0   \n",
       "...     ...  ...        ...        ...        ...       ...       ...   \n",
       "9995    0.0  ...    28847.0    28747.0    29177.0    2000.0    2000.0   \n",
       "9996   -2.0  ...        0.0        0.0        0.0     712.0       0.0   \n",
       "9997   -1.0  ...     3859.0      415.0      415.0  144047.0     365.0   \n",
       "9998   -1.0  ...     1559.0     -205.0      613.0    3000.0       0.0   \n",
       "9999    0.0  ...    47562.0    43735.0    43509.0    3020.0    5040.0   \n",
       "\n",
       "      PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0       4000.0    3000.0    1000.0    2000.0                           0  \n",
       "1       2828.0       0.0     182.0       0.0                           1  \n",
       "2       2200.0    1300.0   43987.0    1386.0                           0  \n",
       "3       1600.0       0.0    1600.0    1600.0                           1  \n",
       "4       2000.0       0.0    2000.0    2000.0                           0  \n",
       "...        ...       ...       ...       ...                         ...  \n",
       "9995    2000.0    2000.0    2000.0    2000.0                           0  \n",
       "9996       0.0       0.0       0.0       0.0                           0  \n",
       "9997   73000.0     415.0     415.0   50136.0                           1  \n",
       "9998    3000.0       0.0    2000.0    1000.0                           0  \n",
       "9999    2000.0    2000.0   12000.0    5000.0                           0  \n",
       "\n",
       "[10000 rows x 24 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the Dataframe\n",
    "import pandas as pd\n",
    "\n",
    "#Name dataframe df\n",
    "df = pd.read_excel('assignment_data/credit_data.xlsx')\n",
    "\n",
    "#Read the first 10,000 rows\n",
    "df.head(10000)\n",
    "\n",
    "#Delete ID column\n",
    "del df['ID']\n",
    "\n",
    "#check new dataframe\n",
    "df.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. List which *features* are *numeric*, *ordinal*, and *nominal* variables, and how many features of each kind there are in the dataset.\n",
    "To answer this question \n",
    "- Find the definitions of the variables provided elsewhere in the course material (hint: make sure you do weekly tutorials)\n",
    "- Find the definitions of numeric, ordinal and nominal variables\n",
    "- Carefully consider the values of the data itself as well as the output of `df.info()`. \n",
    "\n",
    "Your answer should be written up in Markdown and include:\n",
    "1) Definitions of the three kinds of variables,\n",
    "2) A table listing all the features present in the dataset and their type (fill out the table template provided below) and\n",
    "3) A brief description of the contents of the table.\n",
    "\n",
    "|Variable Kind|Number of Features|Feature Names\n",
    "| --- | --- | --- |\n",
    "| Numeric | some number | some text |\n",
    "| some text  | some number | some text |\n",
    "| some text  | some number | some text |\n",
    " \n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**1. Definitions of variables:**\n",
    "\n",
    "**a. Numeric variables**\n",
    "represent quantitative variables that can be measured and   expressed as numbers. They are categorized as either continuous or discrete:\n",
    "\n",
    "- **Continuous variables** represent measurements that can assume any value within a range, allowing for infinite precision. Common examples are height and temperature.\n",
    "\n",
    "- **Discrete variables** count occurrences in whole numbers, forbidding fractional values. Examples include the number of cars a person owns or the number of employees in a company.\n",
    "\n",
    "**b. Nominal variables**\n",
    "are categorical variables that do not have a logical order. These variables identify differences based on type rather than degree. Examples include gender, business type, eye color, religion, and brand.\n",
    "\n",
    "**c. Ordinal variables**\n",
    "are categorical variables that have have a logical order or ranking among the categories. Although these categories can be arranged hierarchically, the distances between them are not quantified. Examples include academic grades (e.g., A, B, C), clothing sizes (e.g., S, M, L, XL), and attitudes (e.g., strongly agree to strongly disagree).\n",
    "\n",
    "Reference:\n",
    "Australian Bureau of Statistics. (n.d.). Variables. Australian Bureau of Statistics. Retrieved from https://www.abs.gov.au/statistics/understanding-statistics/statistical-terms-and-concepts/variables\n",
    "\n",
    "\n",
    "\n",
    "**2. A table listing all the features present in the dataset and their type**\n",
    "\n",
    "\n",
    "|Variable Kind|Number of Features|Feature Names\n",
    "| --- | --- | --- |\n",
    "| Numeric | 14 | LIMIT_BAL, AGE, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6  |\n",
    "| Nominal  | 9 | SEX, MARRIAGE, PAY_0, PAY_2 , PAY_3, PAY_4, PAY_5, PAY_6, default.payment.next.month |\n",
    "| Ordinal  | 1 | EDUCATION |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**3. Brief Explanation of Table**\n",
    "\n",
    "\n",
    "The table categorizes features (variables) in the \"credit_data\" dataset according to their types: numeric, nominal, and ordinal.\n",
    "\n",
    "- There are **14 numeric features**: LIMIT_BAL, AGE, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, and BILL_AMT6. They represent measurable quantities such as balance limits, age, and bill amounts for six different time periods.\n",
    "\n",
    "- There are **9 nominal features**: SEX, MARRIAGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, and default.payment.next.month. These represent categories without a natural order, such as gender, education level, marital status, repayment status for several periods, and whether there was a default payment the following month.\n",
    "\n",
    "- There is only **1 ordinal feature**: Education which represents the level of education.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3.** Missing Values. \n",
    "\n",
    "- Print out the number of missing values for each variable in the dataset and comment on your findings.\n",
    "\n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIMIT_BAL                       0\n",
      "SEX                           383\n",
      "EDUCATION                     383\n",
      "MARRIAGE                      383\n",
      "AGE                           383\n",
      "PAY_0                         383\n",
      "PAY_2                         383\n",
      "PAY_3                         383\n",
      "PAY_4                         358\n",
      "PAY_5                         358\n",
      "PAY_6                         358\n",
      "BILL_AMT1                     358\n",
      "BILL_AMT2                     358\n",
      "BILL_AMT3                     358\n",
      "BILL_AMT4                     360\n",
      "BILL_AMT5                     360\n",
      "BILL_AMT6                     360\n",
      "PAY_AMT1                      360\n",
      "PAY_AMT2                      360\n",
      "PAY_AMT3                        2\n",
      "PAY_AMT4                        2\n",
      "PAY_AMT5                      300\n",
      "PAY_AMT6                      300\n",
      "default payment next month      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Print missing values per column\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All columns contain missing values, except for \"LIMIT_BAL\" and \"default payment next month\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Problem 2.** Cleaning data and dealing with categorical features (Total Marks: 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** \n",
    "\n",
    "- Use an appropriate `pandas` function to impute missing values using one of the following two strategies: `mean` and `mode`. (10 marks)\n",
    "    - Take into consideration the type of each variable  and the best practices we discussed in class/lecture notes\n",
    "- Explain what data imputation is, how you have done it here, and what decisions you had to make. (5 marks)\n",
    "\n",
    "\n",
    "\n",
    "(Total: 15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to impute with mode\n",
    "columns_mode = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "\n",
    "# Impute with mode\n",
    "for column in columns_mode:\n",
    "    mode_value = df[column].mode()[0]  \n",
    "    df[column].fillna(mode_value, inplace=True)\n",
    "\n",
    "# List of columns to impute with mean\n",
    "columns_mean = ['AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "\n",
    "# Impute with mean\n",
    "df[columns_mean] = df[columns_mean].fillna(df[columns_mean].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data imputation is the process of preserving the majority of data by substituting missing values with reasonable values. In the code above, I replaced all missing values on numeric columns with their mean, and for categorical columns, I replaced missing values with their mode. I decided to use mode as it is more fitting for categorical values, while mean is more fitting for numeric values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. \n",
    "- Print `value_counts()` of the 'SEX' column and add a dummy variable named 'SEX_FEMALE' to `df` using `get_dummies()` (3 marks)\n",
    "- Carefully explain what the values of the new variable 'SEX_FEMALE' mean (2 mark)\n",
    "- Make sure the variable 'SEX' is deleted from `df`\n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0    6411\n",
      "1.0    3934\n",
      "Name: SEX, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print value count of SEX Column\n",
    "print(df['SEX'].value_counts())\n",
    "\n",
    "#Create dummy variables for the 'SEX' column\n",
    "one_hot = pd.get_dummies(df['SEX'], prefix='SEX')\n",
    "\n",
    "# Select only the 'SEX_2' column, which corresponds to 'SEX_FEMALE'\n",
    "df['SEX_FEMALE'] = one_hot['SEX_2.0']\n",
    "\n",
    "# delete SEX column\n",
    "del df['SEX']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"SEX_FEMALE\" column serves as an indicator of gender, where a value of 1 denotes Female, and 0 signifies otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\">\n",
    "\n",
    "**Q3**. Print `value_counts()` of the 'MARRIAGE' column and *carefully* comment on what you notice in relation to the definition of this variable. \n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0    5677\n",
      "1.0    4537\n",
      "3.0     113\n",
      "0.0      18\n",
      "Name: MARRIAGE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['MARRIAGE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value_counts() output for the 'MARRIAGE' column indicates that singles (2.0) are the most common group in the dataset with 5,677 occurrences, followed by married individuals (1.0) with 4,537 occurrences. The category labeled 'others' (3.0) represents a small fraction with 113 occurrences, and there are 18 records with a '0.0' value, which is not defined in the given categories of marital status and might indicate missing or improperly coded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\">\n",
    "\n",
    "**Q4**. \n",
    "\n",
    "- Apply `get_dummies()` to 'MARRIAGE' feature and add dummy variables 'MARRIAGE_MARRIED', 'MARRIAGE_SINGLE', 'MARRIAGE_OTHER' to `df`. (5 marks)   \n",
    "- Carefully consider how to allocate all the values of 'MARRIAGE' across these 3 newly created features (5 marks)   \n",
    "    - Explain what decisions you had to make\n",
    "- Make sure that 'MARRIAGE' is deleted from `df`   \n",
    "\n",
    "(Total: 10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change 0 values to mode of \"Marriage\"\n",
    "df['MARRIAGE'].replace(0.0, df['MARRIAGE'].mode()[0], inplace=True)\n",
    "\n",
    "# Create dummy variables for the 'MARRIAGE' column\n",
    "one_hot_marriage = pd.get_dummies(df['MARRIAGE'], prefix='MARRIAGE')\n",
    "\n",
    "# Map the columns\n",
    "marriage_mapping = {\n",
    "    'MARRIAGE_1.0': 'MARRIAGE_MARRIED',  \n",
    "    'MARRIAGE_2.0': 'MARRIAGE_SINGLE',\n",
    "    'MARRIAGE_3.0': 'MARRIAGE_OTHERS'\n",
    "}\n",
    "\n",
    "# Rename the columns using the mapping\n",
    "one_hot_marriage = one_hot_marriage.rename(columns=marriage_mapping)\n",
    "\n",
    "# Step 3: Join the modified dummy DataFrame with the original DataFrame\n",
    "df = df.join(one_hot_marriage)\n",
    "\n",
    "# Delete the 'MARRIAGE' Column\n",
    "del df['MARRIAGE']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key decisions I made are:\n",
    "\n",
    "1. Replace 0 values in 'MARRIAGE' with its mode to ensure there are no undefined or ambiguous entries in the column, making the dataset cleaner and more consistent.\n",
    "   \n",
    "2. Convert 'MARRIAGE' to dummy variables using pd.get_dummies() to transform the categorical data into a numerical format that can be easily used in analysis and modeling.\n",
    "\n",
    "3. Rename dummy variables to meaningful names -'MARRIAGE_MARRIED', 'MARRIAGE_SINGLE', 'MARRIAGE_OTHERS' - to clearly indicate what each variable represents, making the data easier to understand and work with.\n",
    "\n",
    "4. Merge the dummy variables back into the dataset and remove the original 'MARRIAGE' column to update the dataset to include the new, more analytically useful variables while eliminating redundancy by removing the original column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q5**. In the column 'EDUCATION', convert the values {0, 5, 6} to the value 4. \n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace 0,5,6 to 4:\n",
    "df.loc[df['EDUCATION'].isin([0, 5, 6]), 'EDUCATION'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 3** Preparing X and y arrays (Total Marks: 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Create a numpy array `y` from the first 7,500 observations of 'payment_default' column from `df` (2.5 marks)   \n",
    "- Create a numpy array `X`  from the first 7,500 observations of all the remaining variables in `df` (2.5 marks)   \n",
    "\n",
    "(Total: 5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df = df.rename(columns={'PAY_0': 'PAY_1', 'default payment next month': 'payment_default'})\n",
    "\n",
    "# Define the target variable 'y' and feature matrix 'X' using the first 7500 observations\n",
    "y = df.loc[:7499, 'payment_default'].values\n",
    "X = df.loc[:7499, ['LIMIT_BAL', 'EDUCATION', 'AGE', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4',\n",
    "                   'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n",
    "                   'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n",
    "                   'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
    "                   'SEX_FEMALE', 'MARRIAGE_MARRIED', 'MARRIAGE_SINGLE', 'MARRIAGE_OTHERS']].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. \n",
    "\n",
    "- Use an appropriate `sklearn` library we used in class to create `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 75% train and 25% test datasets (2.5 marks) \n",
    "    - Set random_state to 3 and stratify the subsamples so that train and test datasets have roughly equal proportions of the target's class labels \n",
    "- Standardise the data to mean zero and variance one using an approapriate `sklearn` library (2.5 marks)   \n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3, stratify=y)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit to data, then transform it\n",
    "\n",
    "# Standardize the testing data\n",
    "X_test_scaled = scaler.transform(X_test)  # Perform standardization by centering and scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 4**. Support Vector Classifier and Accuracies (Total Marks: 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Train a Support Vector Classifier on the standardised data (5 marks)\n",
    "    - Use `rbf` kernel and set `random_state` to 24 (don't change any other parameters)\n",
    "- Compute and print training and test dataset accuracies (5 marks)\n",
    "\n",
    "(Total: 10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8231111111111111\n",
      "Test Accuracy: 0.8144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the Support Vector Classifier with RBF kernel and random_state set to 24\n",
    "svc = SVC(kernel='rbf', random_state=24)\n",
    "\n",
    "# Train the SVC on the standardized training data\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compute the accuracies on the training and test sets\n",
    "training_accuracy = svc.score(X_train_scaled, y_train)\n",
    "test_accuracy = svc.score(X_test_scaled, y_test)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f'Training Accuracy: {training_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2.**\n",
    "\n",
    "- Extract 2 linear principal components from the standardised features using an appropriate `sklearn` library (5 marks)\n",
    "- Train a Support Vector Classifier on the 2 principal components computed above (5 marks)   \n",
    "    - Use `rbf` kernel and set `random_state` to 24 (don't change any other parameters)\n",
    "- Compute and print training and test dataset accuracies (5 marks)\n",
    "\n",
    "\n",
    "(Total: 15 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset accuracy with PCA: 0.8012444444444444\n",
      "Test dataset accuracy with PCA: 0.8037333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract 2 linear principal components from the standardized data\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Train the SVC on the PCA-transformed data\n",
    "svc_pca = SVC(kernel='rbf', random_state=24)\n",
    "svc_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict the labels for both the training and test sets\n",
    "y_train_pred_pca = svc_pca.predict(X_train_pca)\n",
    "y_test_pred_pca = svc_pca.predict(X_test_pca)\n",
    "\n",
    "# Calculate the accuracy of predictions against the actual labels\n",
    "train_accuracy_pca = accuracy_score(y_train, y_train_pred_pca)\n",
    "test_accuracy_pca = accuracy_score(y_test, y_test_pred_pca)\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Training dataset accuracy with PCA:\", train_accuracy_pca)\n",
    "print(\"Test dataset accuracy with PCA:\", test_accuracy_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3**. \n",
    "\n",
    "- Comment on the suitability of the two classifiers to predict credit card defaults by commenting on (and comparing) the computed accuracies from the last two questions.\n",
    "\n",
    "\n",
    "(Total: 5 marks)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying and comparing two methods for predicting credit card defaults, I observed interesting results. Method A, which utilized a Support Vector Classifier (SVC) directly on the standardized data, yielded a training accuracy of 82.31% and a test accuracy of 81.44%. Method B, on the other hand, involved using PCA to reduce the feature space before applying SVC, resulting in slightly lower accuracies: a training accuracy of 80.12% and a test accuracy of 80.37%.\n",
    "\n",
    "The higher accuracies in Method A suggest that the SVC benefits from the complexity and richness of the full feature set, capturing more nuances in the data that are crucial for predicting credit card defaults. This could indicate that the original feature set contains valuable information for the classification task that is lost during the dimensionality reduction process in Method B. However,  The fact that Method B still maintains relatively high accuracy with only two principal components suggests that these components capture a significant portion of the variance necessary for the classification task. In summary, if the goal is to maximize predictive accuracy and the computational resources are available, Method A is preferable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Criteria\n",
    "\n",
    "To achieve a perfect score, your solutions must adhere to the criteria outlined below:\n",
    "\n",
    "- Ensure that all numerical answers are accurate.\n",
    "- Use the specific Python functions and libraries specified within the assignment instructions.\n",
    "- For any written responses, provide accurate information, articulated in clear and complete sentences.\n",
    "- Do not add extra cells beyond what is provided in the notebook.\n",
    "- Do not print output with your code unless explicitly instructed to do so.\n",
    "- Maintain a clean and organised notebook layout that is easy to follow.\n",
    "    \n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
